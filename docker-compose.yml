services:

  pipelines:
    image: ghcr.io/open-webui/pipelines:main
    container_name: pipelines
    depends_on:
      - ollama
    ports:
      - "9099:9099"
    volumes:
      - ./pipelines:/app/pipelines
      # gleiche DBs wie bei sqlite-tools, read-only
      - ./dbs:/data:ro
    environment:
      # Ollama läuft in deinem Compose, daher service-name "ollama"
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_MODEL=llama3.1:latest
      - TIMEOUT_S=600
      # wo die DBs im Container liegen
      - DBS_ROOT=/data

      # Limits (Sicherheit + Performance)
      - MAX_ROWS_DEFAULT=50
      - MAX_ROWS_HARD=500

      # optional: schema-cache (sekunden)
      - SCHEMA_CACHE_SECONDS=600

      # API Key für Open WebUI Connection (optional, aber gut)
      - PIPELINES_API_KEY=pipelines123
    restart: unless-stopped

  openwebui:
    build: ./openwebui-custom
    ports:
      - "3000:8080"
    volumes:
      - open-webui:/app/backend/data
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      # OpenAI-kompatibler Provider -> dein Pipelines-Server
      - OPENAI_API_BASE_URL=http://pipelines:9099/v1
      - OPENAI_API_KEY=${PIPELINES_API_KEY}
      # Admin automatisch anlegen (nur beim allerersten Start, wenn noch keine User existieren)
      - WEBUI_ADMIN_EMAIL=${WEBUI_ADMIN_EMAIL}
      - WEBUI_ADMIN_PASSWORD=${WEBUI_ADMIN_PASSWORD}
      - WEBUI_ADMIN_NAME=${WEBUI_ADMIN_NAME}
    depends_on:
      - ollama
      - pipelines

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama:/root/.ollama

volumes:
  open-webui:
  ollama:
