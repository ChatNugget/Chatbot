services:

  pipelines:
    image: ghcr.io/open-webui/pipelines:main
    container_name: pipelines
    depends_on:
      - ollama
    ports:
      - "9099:9099"
    volumes:
      - ./pipelines:/app/pipelines
      - ./dbs:/data:ro
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_MODEL=llama3.1:latest
      - OLLAMA_KEEP_ALIVE=30m

      - TIMEOUT_S=600
      - DBS_ROOT=/data

      - MAX_ROWS_DEFAULT=50
      - MAX_ROWS_HARD=500
      - CACHE_SECONDS=3600

      # Fast routing defaults (no LLM router)
      - ALLOW_LLM_ROUTER=0
      - ROUTER_TOP_K=8
      - HEURISTIC_MIN_SCORE=2
      - HEURISTIC_RATIO=1.5
      - HEURISTIC_MARGIN=2

      # Slim schema (big win)
      - SCHEMA_TOP_TABLES=6
      - SCHEMA_MAX_COLS=25

      # Keep prompts small
      - USE_LAST_USER_MESSAGE_ONLY=1
      - QUESTION_MAX_CHARS=1200
      - QUESTION_KEEP_LAST_LINES=12
      - STRIP_PIPELINE_OUTPUT_NOISE=1

      # LLM output cap
      - SQL_NUM_PREDICT=128

      # Timing
      - TIMING=1
      - TIMING_LOG_SQL=0
      - TIMING_LOG_PROMPT_CHARS=1
      - TIMING_LOG_OLLAMA_METRICS=1

      - PIPELINES_API_KEY=pipelines123
    restart: unless-stopped

  openwebui:
    image: ghcr.io/open-webui/open-webui:${OPENWEBUI_TAG}
    expose:
      - "8080"
    volumes:
      - open-webui:/app/backend/data
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - OPENAI_API_BASE_URL=http://pipelines:9099/v1
      - OPENAI_API_KEY=${PIPELINES_API_KEY}
      - WEBUI_ADMIN_EMAIL=${WEBUI_ADMIN_EMAIL}
      - WEBUI_ADMIN_PASSWORD=${WEBUI_ADMIN_PASSWORD}
      - WEBUI_ADMIN_NAME=${WEBUI_ADMIN_NAME}
    depends_on:
      - ollama
      - pipelines

  proxy:
    image: nginx:alpine
    depends_on:
      - openwebui
    ports:
      - "${WEBUI_PORT}:80"
    volumes:
      - ./nginx.conf:/etc/nginx/conf.d/default.conf:ro
      - ./openwebui-custom/custom.js:/usr/share/nginx/html/custom.js:ro
    restart: unless-stopped

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama:/root/.ollama

volumes:
  open-webui:
  ollama:
